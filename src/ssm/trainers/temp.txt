'''
    if train_config['load_model']:
        try:
            print(train_config['checkpoint'])
            custom_loss_trained_path = train_config['load_checkpoint'].format(loss_fn=loss_name)
            model = get_ssm_model_attention(checkpoint=custom_loss_trained_path)
            print("Loading model from checkpoint...")
            #checkpoint_path = rf"C:\Users\CL-11\OneDrive\Repos\OCTDenoisingFinal\ssm\checkpoints\{repr(model)}_best.pth"
            
            checkpoint_path = train_config['load_checkpoint'].format(loss_fn=loss_name)
            
            checkpoint = torch.load(checkpoint_path, map_location=device)
            model.load_state_dict(checkpoint['model_state_dict'])
            model.to(device)
            optimizer = optim.Adam(model.parameters(), lr=learning_rate)
            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            best_loss = checkpoint['best_loss']
            set_epoch = checkpoint['epoch']
            history = checkpoint['history']
            num_epochs = num_epochs + set_epoch
            print(f"Model loaded from {checkpoint_path} at epoch {set_epoch} with loss {best_loss:.6f}")
        except Exception as e:
            print(f"Error loading model: {e}")
            print("Starting training from scratch.")
            raise e 
    else:
        model = get_ssm_model(checkpoint=None)
        model.to(device)
        optimizer = optim.Adam(model.parameters(), lr=learning_rate)
        best_loss = float('inf')
        set_epoch = 0
    '''